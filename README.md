Scaled Dot-Product Attention from Scratch using NumPy
# Scaled Dot-Product Attention from Scratch (NumPy)

Why This Project Matters

Modern NLP models such as Transformers, BERT, and GPT rely heavily on attention mechanisms.
This project shows that I understand how attention works internally, not just how to call library APIs.

Key focus areas:

Mathematical formulation of attention

Efficient matrix computations

Batch processing

Mask handling

Interpretability via visualization

ðŸ§  Concepts Demonstrated

Scaled Dot-Product Attention

Softmax normalization with numerical stability

Attention masking (padding support)

Batched tensor operations

Model interpretability through heatmaps

âœ¨ Features

Implemented entirely using NumPy

Supports batched inputs

Supports attention masking

Clean and modular code

Visualizes attention weights using heatmaps

Fully documented Jupyter Notebook

ðŸ›  Tech Stack

Language: Python 3.x

Libraries: NumPy, Matplotlib

Environment: Jupyter Notebook

---

 ##Features
- Pure NumPy implementation (no deep learning frameworks)
- Supports **batched inputs**
- Supports **attention masking**
- Heatmap visualization of attention weights
- Well-documented Jupyter Notebook

---

## Technologies Used
- Python 3.x
- NumPy
- Matplotlib
- Jupyter Notebook

---
Future Improvements

Multi-Head Attention

Positional Encoding

Causal (look-ahead) masking

Full Transformer block implementation

Integration with pretrained embeddings

Ideal For

Machine Learning Internships

NLP / AI Research Internships

Software Engineering roles involving ML systems


## ðŸ“‚ Repository Structure
scaled-dot-product-attention/
â”‚
â”œâ”€â”€ Scaled_Dot_Product_Attention.ipynb  
â”œâ”€â”€ README.md                            
â”œâ”€â”€ requirements.txt  


How to Run:
pip install -r requirements.txt
jupyter notebook
Open Scaled_Dot_Product_Attention.ipynb and run all cells.

